// AUTO-GENERATED FILE - DO NOT EDIT
// Generated by scripts/build-project-content.ts

export interface ProjectLink {
  label: string;
  url: string;
}

export interface Project {
  slug: string;
  title: string;
  summary: string;
  tags: string[];
  featured: boolean;
  tech: string[];
  links: ProjectLink[];
  previewImageUrl?: string;
  previewVideoUrl?: string;
  order?: number;
  draft: boolean;
  html: string;
  text: string;
  assets: string[];
}

export const projects: Project[] = [
  {
    "slug": "building-trust-in-ai",
    "title": "Explainable GNNs for Anti-Money Laundering Detection",
    "summary": "Graduate thesis developing transparent Graph Neural Networks for cryptocurrency AML detection using the Elliptic2 dataset. Implementing multi-method explainability to make black-box models interpretable for financial compliance.",
    "tags": [
      "Graph Neural Networks",
      "XAI",
      "Financial Crime",
      "Deep Learning",
      "Research"
    ],
    "featured": true,
    "tech": [
      "Python",
      "PyTorch Geometric",
      "scikit-learn",
      "Post-hoc explainability",
      "NetworkX",
      "Pandas",
      "NumPy",
      "Large Data"
    ],
    "links": [],
    "previewImageUrl": "/projects/building-trust-in-ai_preview_8f77a7c9ae94.webp",
    "order": 1,
    "draft": false,
    "html": "<h1>Building Trust in AI: Explainable Graph Neural Networks for Financial Crime Detection</h1>\n<h2>The Challenge</h2>\n<p>Every year, up to 5% of global GDP, approximately €1.7 trillion—is laundered through the financial system. Financial institutions invest heavily in combating this threat, with compliance costs in the EMEA region alone reaching $85 billion in 2023. Despite these massive investments, Anti-Money Laundering (AML) systems struggle with high false-positive rates and limited interpretability, creating both operational inefficiency and regulatory challenges.</p>\n<p>Traditional machine learning approaches treat transactions in isolation, missing the crucial relational patterns that characterize financial crime. While Graph Neural Networks (GNNs) can model these network relationships, they operate as &quot;black boxes&quot;—making it difficult to understand why a particular transaction was flagged as suspicious. For regulated financial institutions, this lack of transparency is a critical barrier to adoption.</p>\n<hr>\n<h2><img src=\"/projects/building-trust-in-ai_diagram_8f77a7c9ae94.webp\" alt=\"Topologies\"></h2>\n<h2>My Research: Bringing Explainability to Graph-Based AML Detection</h2>\n<p>My thesis explores how to make GNN-based financial crime detection not just accurate, but also transparent and interpretable. I&#39;m developing and evaluating explainable GNN models using the Elliptic2 dataset—a cutting-edge benchmark created by MIT CSAIL, IBM Watson AI Lab, and Elliptic that contains real-world cryptocurrency transaction networks.</p>\n<h3>The Dataset Challenge</h3>\n<p>The Elliptic2 dataset presents significant technical challenges that mirror real-world AML scenarios:</p>\n<ul>\n<li><strong>Scale</strong>: 49 million background nodes and 445,000 labeled nodes organized into 122,000 transaction subgraphs</li>\n<li><strong>Severe class imbalance</strong>: Only 2.3% of transactions are labeled as suspicious—reflecting the reality that illicit activity is relatively rare but high-impact</li>\n<li><strong>Feature anonymization</strong>: 43 binned features representing cluster-level statistics, network topology, and temporal patterns, designed to protect intellectual property while maintaining research utility</li>\n<li><strong>Graph structure</strong>: Transactions are modeled as subgraphs rather than isolated points, preserving the relational context critical for detecting money laundering patterns</li>\n</ul>\n<h2>Preprocessing &amp; Baseline Establishment</h2>\n<p>The preprocessing pipeline transforms raw Elliptic2 data into research-ready artifacts with reproducible quality controls:</p>\n<p><strong>Graph Construction</strong>\nNode re-indexing ensures contiguous structures, while validation checks maintain edge-node consistency and intra-subgraph label integrity. Raw data is converted to Parquet for efficient access.</p>\n<p><strong>Feature Handling</strong>\nAligned feature matrices preserve the ordinal nature of anonymized binned features. Missing values are handled to maintain feature semantics, with all statistics computed exclusively on training data.</p>\n<p><strong>Experimental Design</strong>\nStratified train/val/test splits are performed at the subgraph level to prevent structural leakage while maintaining class distribution across the severe imbalance (2.3% positive rate). All splits and mappings are persisted for reproducibility.</p>\n<p><strong>Evaluation Framework</strong>\nBaseline comparisons use Precision-Recall AUC as the primary metric, appropriate for the extreme class imbalance where standard accuracy is uninformative.</p>\n<h2>What I&#39;m Building Now: Multi-Method Explainability Framework</h2>\n<p>I&#39;m currently implementing a comprehensive explainability layer that will make GNN predictions interpretable through multiple complementary approaches:</p>\n<p><strong>Feature-Based Explainability</strong></p>\n<ul>\n<li><strong>SHAP (SHapley Additive exPlanations)</strong>: Quantifying how much each feature contributes to individual predictions using game-theoretic principles</li>\n<li><strong>LIME (Local Interpretable Model-agnostic Explanations)</strong>: Creating local linear approximations around specific predictions to understand decision boundaries</li>\n</ul>\n<p><strong>Graph-Native Explainability</strong></p>\n<ul>\n<li><strong>GNNExplainer</strong>: Identifying which subgraph structures and node neighborhoods are most important for each prediction</li>\n<li><strong>Counterfactual-GNN</strong>: Generating minimal graph modifications that would flip a prediction, revealing the decision-critical patterns</li>\n</ul>\n<p><strong>Topological Pattern Analysis</strong>\nI&#39;m particularly focused on analyzing how transaction network topology influences detection—recognizing that structural patterns (like star networks, long chains, or dense clusters) often indicate specific laundering techniques.</p>\n<h2>Research Goals &amp; Impact</h2>\n<p>This work addresses four key objectives:</p>\n<ol>\n<li><strong>Performance</strong>: Comparing GNN subgraph classification against traditional ML baselines to quantify the value of graph-based approaches</li>\n<li><strong>Explainability</strong>: Evaluating multiple explanation methods for transparency, stability, and practical utility in AML workflows</li>\n<li><strong>Fairness &amp; Bias</strong>: Investigating whether predictions and explanations differ across structural subgroups to identify potential biases</li>\n<li><strong>Practical Integration</strong>: Designing explanation outputs that can realistically support AML analyst decision-making</li>\n</ol>\n<p>The ultimate goal is to demonstrate that advanced AI can be both powerful <em>and</em> trustworthy in high-stakes financial compliance applications.</p>\n<h2>Why This Matters</h2>\n<p>Explainable AI in financial compliance isn&#39;t just an academic exercise—it&#39;s a practical necessity. Regulatory frameworks increasingly require financial institutions to explain automated decisions. Analysts need to understand why transactions are flagged to investigate efficiently. And ultimately, effective AML systems must balance detection capability with operational feasibility.</p>\n<p>By developing rigorous methods for explaining GNN predictions on financial networks, this research aims to accelerate the adoption of graph-based AI in financial crime detection—bringing more sophisticated tools to bear on a critical global challenge while maintaining the transparency required for responsible deployment.</p>\n<hr>\n<p><em>This work is conducted as part of my graduate thesis in Computing with Artificial Intelligence &amp; Machine Learning at TU Dublin</em></p>\n",
    "text": "Building Trust in AI: Explainable Graph Neural Networks for Financial Crime Detection The Challenge Every year, up to 5% of global GDP, approximately €1.7 trillion—is laundered through the financial system. Financial institutions invest heavily in combating this threat, with compliance costs in the EMEA region alone reaching $85 billion in 2023. Despite these massive investments, Anti-Money Laundering (AML) systems struggle with high false-positive rates and limited interpretability, creating both operational inefficiency and regulatory challenges. Traditional machine learning approaches treat transactions in isolation, missing the crucial relational patterns that characterize financial crime. While Graph Neural Networks (GNNs) can model these network relationships, they operate as &quot;black boxes&quot;—making it difficult to understand why a particular transaction was flagged as suspicious. For regulated financial institutions, this lack of transparency is a critical barrier to adoption. My Research: Bringing Explainability to Graph-Based AML Detection My thesis explores how to make GNN-based financial crime detection not just accurate, but also transparent and interpretable. I&#39;m developing and evaluating explainable GNN models using the Elliptic2 dataset—a cutting-edge benchmark created by MIT CSAIL, IBM Watson AI Lab, and Elliptic that contains real-world cryptocurrency transaction networks. The Dataset Challenge The Elliptic2 dataset presents significant technical challenges that mirror real-world AML scenarios: Scale : 49 million background nodes and 445,000 labeled nodes organized into 122,000 transaction subgraphs Severe class imbalance : Only 2.3% of transactions are labeled as suspicious—reflecting the reality that illicit activity is relatively rare but high-impact Feature anonymization : 43 binned features representing cluster-level statistics, network topology, and temporal patterns, designed to protect intellectual property while maintaining research utility Graph structure : Transactions are modeled as subgraphs rather than isolated points, preserving the relational context critical for detecting money laundering patterns Preprocessing &amp; Baseline Establishment The preprocessing pipeline transforms raw Elliptic2 data into research-ready artifacts with reproducible quality controls: Graph Construction Node re-indexing ensures contiguous structures, while validation checks maintain edge-node consistency and intra-subgraph label integrity. Raw data is converted to Parquet for efficient access. Feature Handling Aligned feature matrices preserve the ordinal nature of anonymized binned features. Missing values are handled to maintain feature semantics, with all statistics computed exclusively on training data. Experimental Design Stratified train/val/test splits are performed at the subgraph level to prevent structural leakage while maintaining class distribution across the severe imbalance (2.3% positive rate). All splits and mappings are persisted for reproducibility. Evaluation Framework Baseline comparisons use Precision-Recall AUC as the primary metric, appropriate for the extreme class imbalance where standard accuracy is uninformative. What I&#39;m Building Now: Multi-Method Explainability Framework I&#39;m currently implementing a comprehensive explainability layer that will make GNN predictions interpretable through multiple complementary approaches: Feature-Based Explainability SHAP (SHapley Additive exPlanations) : Quantifying how much each feature contributes to individual predictions using game-theoretic principles LIME (Local Interpretable Model-agnostic Explanations) : Creating local linear approximations around specific predictions to understand decision boundaries Graph-Native Explainability GNNExplainer : Identifying which subgraph structures and node neighborhoods are most important for each prediction Counterfactual-GNN : Generating minimal graph modifications that would flip a prediction, revealing the decision-critical patterns Topological Pattern Analysis I&#39;m particularly focused on analyzing how transaction network topology influences detection—recognizing that structural patterns (like star networks, long chains, or dense clusters) often indicate specific laundering techniques. Research Goals &amp; Impact This work addresses four key objectives: Performance : Comparing GNN subgraph classification against traditional ML baselines to quantify the value of graph-based approaches Explainability : Evaluating multiple explanation methods for transparency, stability, and practical utility in AML workflows Fairness &amp; Bias : Investigating whether predictions and explanations differ across structural subgroups to identify potential biases Practical Integration : Designing explanation outputs that can realistically support AML analyst decision-making The ultimate goal is to demonstrate that advanced AI can be both powerful and trustworthy in high-stakes financial compliance applications. Why This Matters Explainable AI in financial compliance isn&#39;t just an academic exercise—it&#39;s a practical necessity. Regulatory frameworks increasingly require financial institutions to explain automated decisions. Analysts need to understand why transactions are flagged to investigate efficiently. And ultimately, effective AML systems must balance detection capability with operational feasibility. By developing rigorous methods for explaining GNN predictions on financial networks, this research aims to accelerate the adoption of graph-based AI in financial crime detection—bringing more sophisticated tools to bear on a critical global challenge while maintaining the transparency required for responsible deployment. This work is conducted as part of my graduate thesis in Computing with Artificial Intelligence &amp; Machine Learning at TU Dublin",
    "assets": [
      "/projects/building-trust-in-ai_diagram_8f77a7c9ae94.webp",
      "/projects/building-trust-in-ai_preview_8f77a7c9ae94.webp"
    ]
  },
  {
    "slug": "building-an-ai-planning-intelligence-platform-for-ireland",
    "title": "AI Planning Intelligence Platform (Co-founder)",
    "summary": "Designed and led the technical architecture for an AI-enhanced planning intelligence platform focused on the Irish planning system. Built a traceable, explainable AI workspace that combines planning law, policy, precedent and document workflows to reduce validation errors, research overhead and professional risk in regulated decision-making.",
    "tags": [
      "System Architecture",
      "Explainable AI",
      "Domain-Driven Design",
      "Decision Support",
      "Founder-Led Engineering"
    ],
    "featured": true,
    "tech": [
      "TypeScript",
      "Golang",
      "React",
      "SQL",
      "Document Pipelines",
      "RAG",
      "Agentic AI",
      "Containers",
      "CI/CD Pipelines",
      "Cloud Infrastructure"
    ],
    "links": [
      {
        "label": "Company Website",
        "url": "https://www.eireplan.ie/"
      },
      {
        "label": "LinkedIn",
        "url": "https://www.linkedin.com/company/eireplan"
      }
    ],
    "previewVideoUrl": "/projects/building-an-ai-planning-intelligence-platform-for-ireland_preview_video_59c10df13a4a.mp4",
    "order": 2,
    "draft": false,
    "html": "<h1>Building an AI Planning Intelligence Platform for Ireland</h1>\n<h2>Summary</h2>\n<p>This project documents the <strong>design, architecture, and delivery of an AI-enhanced planning intelligence platform that is currently running in live production and used by planning professionals</strong>.</p>\n<p>The platform supports real-world planning workflows by combining Irish planning law, national and local policy, precedent decisions, and document intelligence into a <strong>traceable, explainable decision-support system</strong>. From a technical perspective, the work focuses on building robust document ingestion pipelines, retrieval-augmented reasoning with strict citation control, and workflow-aware AI tooling suitable for regulated environments. From an entrepreneurial perspective, it demonstrates how domain-specific AI infrastructure can be productised and deployed responsibly in a high-accountability professional sector.</p>\n<p>The central principle underpinning the system is simple:</p>\n<p><strong>If the reasoning cannot be traced and explained, the output cannot be used.</strong></p>\n<hr>\n<h2><video controls src=\"/projects/building-an-ai-planning-intelligence-platform-for-ireland_demo_59c10df13a4a.mp4\"></video></h2>\n<h2>Why I Built This</h2>\n<p>Planning is often portrayed as slow or inefficient. After working closely with Irish planning legislation, regulator data, and professional workflows, it became clear that this framing is misleading.</p>\n<p>The Irish planning system is already highly structured, rule-based, and defensible. Planners operate within explicit chains of logic grounded in legislation, development plans, policy objectives, site constraints, and precedent decisions. The problem is not a lack of structure. It is the absence of software that can surface, manage, and reuse that structure effectively.</p>\n<p>Most general-purpose AI tools flatten this complexity into single answers. In a regulated environment where accountability and auditability matter, that makes them unusable.</p>\n<p>This project was built to address that gap: to create a <strong>planning-native AI platform</strong> that strengthens professional judgement rather than obscuring it.</p>\n<hr>\n<h2>The Problem Space</h2>\n<p>Public planning data and regulator reporting highlight several persistent issues:</p>\n<ul>\n<li>High and sustained application volumes</li>\n<li>Significant invalidation rates before assessment</li>\n<li>Increasing legislative and policy complexity</li>\n<li>Large amounts of professional time spent on repetitive research, validation, and rework</li>\n</ul>\n<p>These issues are systemic rather than incidental. From a software engineering perspective, they point to a missing layer of infrastructure: a shared intelligence system that understands planning as a domain, rather than as unstructured text scattered across PDFs and portals.</p>\n<hr>\n<h2>What I Built</h2>\n<h3>A Planning Intelligence Platform in Live Use</h3>\n<p>The result is an <strong>AI-enhanced planning intelligence platform</strong> that is deployed in production and actively used by planners as part of day-to-day work, including research, report preparation, and application validation.</p>\n<p>The system is deliberately not positioned as a chatbot. It is a <strong>decision-support workspace</strong> built around two tightly integrated layers:</p>\n<ol>\n<li>a structured planning knowledge engine  </li>\n<li>a workflow-aware professional interface</li>\n</ol>\n<hr>\n<h2>1. Planning Knowledge Engine</h2>\n<p>The knowledge layer aggregates and normalises authoritative planning sources, including:</p>\n<ul>\n<li>Irish planning legislation and regulations</li>\n<li>National, regional, and local development plans</li>\n<li>Local authority planning registers</li>\n<li>Appeal decisions and outcomes</li>\n</ul>\n<p>Key technical characteristics include:</p>\n<ul>\n<li>Retrieval-augmented generation with strict citation enforcement</li>\n<li>Structured responses rather than free-form text</li>\n<li>Fully inspectable sources for every output</li>\n<li>Explicit prevention of invented authority or uncited reasoning</li>\n</ul>\n<p>Instead of answering questions opaquely, the system exposes the underlying policy logic and references that professionals are required to rely on. This allows outputs to be reviewed, challenged, and defended in professional contexts.</p>\n<hr>\n<h2>2. Workflow-Aware Planning Workspace</h2>\n<p>On top of the knowledge engine, the platform provides tooling that reflects how planning work actually happens.</p>\n<p>This includes:</p>\n<ul>\n<li>Assisted drafting of planning reports and statements</li>\n<li>Context-aware validation and completeness checks</li>\n<li>Discovery of comparable precedent cases</li>\n<li>Planning-specific project stages and statutory timelines</li>\n</ul>\n<p>All AI-assisted outputs are grounded in uploaded drawings, site information, and applicable policy. Users retain full editorial control, and responsibility for final submissions remains explicitly human.</p>\n<p>The platform is designed to reduce cognitive and administrative load, not to automate decision-making.</p>\n<hr>\n<h2>A Core Design Principle: Explainable or Unusable</h2>\n<p>One of the strongest conclusions from building and deploying this system was the necessity of a strict design rule:</p>\n<p><strong>If the logic flow cannot be traced, the output cannot be used.</strong></p>\n<p>This principle shaped every architectural and product decision:</p>\n<ul>\n<li>No black-box conclusions</li>\n<li>No opaque scoring without explanation</li>\n<li>No automation without auditability</li>\n<li>No outputs without explicit sources</li>\n</ul>\n<p>This mirrors best practice in safety-critical and regulated software systems and is essential for maintaining professional trust in live environments.</p>\n<hr>\n<h2>Technical Direction and Architecture</h2>\n<p>Although the platform continues to evolve, its technical foundations are stable and deliberately conservative.</p>\n<p>Core architectural elements include:</p>\n<ul>\n<li>Document ingestion pipelines for legislation, policy, and decisions</li>\n<li>Metadata extraction for policies, constraints, and outcomes</li>\n<li>Domain-specific prompting and templates</li>\n<li>Retrieval-augmented reasoning with citation guarantees</li>\n<li>Clear separation between assistance and decision authority</li>\n</ul>\n<p>The system is designed to scale incrementally, from limited geographic or case-type coverage to broader jurisdictional support, without changing its core reasoning model.</p>\n<hr>\n<h2>Commercial and Entrepreneurial Context</h2>\n<p>This project was developed with commercial realism in mind from the outset.</p>\n<p>The target market is the <strong>knowledge-intensive portion of planning work</strong>: research, validation, drafting, coordination, and precedent analysis. These activities represent significant cost and risk for both private consultants and developers.</p>\n<p>The platform follows a credible adoption path:</p>\n<ul>\n<li>initial deployment with professional planning users,</li>\n<li>expansion across additional authorities and use cases,</li>\n<li>growth into larger multi-office and enterprise contracts,</li>\n<li>and, over time, potential public-sector deployments with appropriate governance.</li>\n</ul>\n<p>The focus throughout has been on delivering tangible professional value rather than speculative automation.</p>\n<hr>\n<h2>Why This Project Matters to Me as a Developer</h2>\n<p>This project reflects the kind of engineering work I am most interested in:</p>\n<ul>\n<li>complex real-world systems,</li>\n<li>regulated decision-making environments,</li>\n<li>and AI used as infrastructure rather than spectacle.</li>\n</ul>\n<p>Building and deploying this platform required careful attention to traceability, accountability, performance, and professional trust. It reinforced the importance of designing AI systems that respect domain constraints and human responsibility.</p>\n<hr>\n<h2>What Comes Next</h2>\n<p>With the platform operating in live production, ongoing work focuses on:</p>\n<ul>\n<li>expanding coverage across additional local authorities and case types,</li>\n<li>incorporating feedback from active professional users,</li>\n<li>strengthening monitoring, reliability, and performance,</li>\n<li>and introducing carefully bounded predictive features with full explainability.</li>\n</ul>\n<p>The objective is not to replace planners or simplify planning beyond recognition, but to provide software that genuinely understands and supports how planning work is done today.</p>\n",
    "text": "Building an AI Planning Intelligence Platform for Ireland Summary This project documents the design, architecture, and delivery of an AI-enhanced planning intelligence platform that is currently running in live production and used by planning professionals . The platform supports real-world planning workflows by combining Irish planning law, national and local policy, precedent decisions, and document intelligence into a traceable, explainable decision-support system . From a technical perspective, the work focuses on building robust document ingestion pipelines, retrieval-augmented reasoning with strict citation control, and workflow-aware AI tooling suitable for regulated environments. From an entrepreneurial perspective, it demonstrates how domain-specific AI infrastructure can be productised and deployed responsibly in a high-accountability professional sector. The central principle underpinning the system is simple: If the reasoning cannot be traced and explained, the output cannot be used. Why I Built This Planning is often portrayed as slow or inefficient. After working closely with Irish planning legislation, regulator data, and professional workflows, it became clear that this framing is misleading. The Irish planning system is already highly structured, rule-based, and defensible. Planners operate within explicit chains of logic grounded in legislation, development plans, policy objectives, site constraints, and precedent decisions. The problem is not a lack of structure. It is the absence of software that can surface, manage, and reuse that structure effectively. Most general-purpose AI tools flatten this complexity into single answers. In a regulated environment where accountability and auditability matter, that makes them unusable. This project was built to address that gap: to create a planning-native AI platform that strengthens professional judgement rather than obscuring it. The Problem Space Public planning data and regulator reporting highlight several persistent issues: High and sustained application volumes Significant invalidation rates before assessment Increasing legislative and policy complexity Large amounts of professional time spent on repetitive research, validation, and rework These issues are systemic rather than incidental. From a software engineering perspective, they point to a missing layer of infrastructure: a shared intelligence system that understands planning as a domain, rather than as unstructured text scattered across PDFs and portals. What I Built A Planning Intelligence Platform in Live Use The result is an AI-enhanced planning intelligence platform that is deployed in production and actively used by planners as part of day-to-day work, including research, report preparation, and application validation. The system is deliberately not positioned as a chatbot. It is a decision-support workspace built around two tightly integrated layers: a structured planning knowledge engine a workflow-aware professional interface 1. Planning Knowledge Engine The knowledge layer aggregates and normalises authoritative planning sources, including: Irish planning legislation and regulations National, regional, and local development plans Local authority planning registers Appeal decisions and outcomes Key technical characteristics include: Retrieval-augmented generation with strict citation enforcement Structured responses rather than free-form text Fully inspectable sources for every output Explicit prevention of invented authority or uncited reasoning Instead of answering questions opaquely, the system exposes the underlying policy logic and references that professionals are required to rely on. This allows outputs to be reviewed, challenged, and defended in professional contexts. 2. Workflow-Aware Planning Workspace On top of the knowledge engine, the platform provides tooling that reflects how planning work actually happens. This includes: Assisted drafting of planning reports and statements Context-aware validation and completeness checks Discovery of comparable precedent cases Planning-specific project stages and statutory timelines All AI-assisted outputs are grounded in uploaded drawings, site information, and applicable policy. Users retain full editorial control, and responsibility for final submissions remains explicitly human. The platform is designed to reduce cognitive and administrative load, not to automate decision-making. A Core Design Principle: Explainable or Unusable One of the strongest conclusions from building and deploying this system was the necessity of a strict design rule: If the logic flow cannot be traced, the output cannot be used. This principle shaped every architectural and product decision: No black-box conclusions No opaque scoring without explanation No automation without auditability No outputs without explicit sources This mirrors best practice in safety-critical and regulated software systems and is essential for maintaining professional trust in live environments. Technical Direction and Architecture Although the platform continues to evolve, its technical foundations are stable and deliberately conservative. Core architectural elements include: Document ingestion pipelines for legislation, policy, and decisions Metadata extraction for policies, constraints, and outcomes Domain-specific prompting and templates Retrieval-augmented reasoning with citation guarantees Clear separation between assistance and decision authority The system is designed to scale incrementally, from limited geographic or case-type coverage to broader jurisdictional support, without changing its core reasoning model. Commercial and Entrepreneurial Context This project was developed with commercial realism in mind from the outset. The target market is the knowledge-intensive portion of planning work : research, validation, drafting, coordination, and precedent analysis. These activities represent significant cost and risk for both private consultants and developers. The platform follows a credible adoption path: initial deployment with professional planning users, expansion across additional authorities and use cases, growth into larger multi-office and enterprise contracts, and, over time, potential public-sector deployments with appropriate governance. The focus throughout has been on delivering tangible professional value rather than speculative automation. Why This Project Matters to Me as a Developer This project reflects the kind of engineering work I am most interested in: complex real-world systems, regulated decision-making environments, and AI used as infrastructure rather than spectacle. Building and deploying this platform required careful attention to traceability, accountability, performance, and professional trust. It reinforced the importance of designing AI systems that respect domain constraints and human responsibility. What Comes Next With the platform operating in live production, ongoing work focuses on: expanding coverage across additional local authorities and case types, incorporating feedback from active professional users, strengthening monitoring, reliability, and performance, and introducing carefully bounded predictive features with full explainability. The objective is not to replace planners or simplify planning beyond recognition, but to provide software that genuinely understands and supports how planning work is done today.",
    "assets": [
      "/projects/building-an-ai-planning-intelligence-platform-for-ireland_demo_59c10df13a4a.mp4",
      "/projects/building-an-ai-planning-intelligence-platform-for-ireland_preview_video_59c10df13a4a.mp4"
    ]
  },
  {
    "slug": "leo",
    "title": "LEO – AI-Powered Anomaly Detection & Automation Service",
    "summary": "Designed and built a production-oriented anomaly detection service exposing a FastAPI-based interface for real-time and batch time-series analysis, with support for configurable thresholds, execution tracking, and extensible ML backends.",
    "tags": [
      "Python",
      "Machine Learning",
      "Anomaly Detection",
      "API",
      "Automation",
      "Real-time"
    ],
    "featured": true,
    "tech": [
      "Python",
      "FastAPI",
      "NumPy",
      "scikit-learn",
      "Redis",
      "Docker"
    ],
    "links": [
      {
        "label": "GitHub Repository",
        "url": "https://github.com/NeriCarcasci/leo"
      }
    ],
    "order": 2,
    "draft": false,
    "html": "<h1>LEO-LOCAL: AI-Powered Natural Language Shell Interface</h1>\n<h2>Vision &amp; Technical Overview</h2>\n<p><strong>LEO-LOCAL</strong> represents an ambitious bridge between natural language processing and system administration, transforming how developers interact with command-line environments. The project envisions a future where shell operations are democratized through conversational AI, eliminating the cognitive overhead of memorizing complex command syntax while maintaining the precision and power of traditional CLIs.</p>\n<hr>\n<h2><img src=\"/projects/leo_image_a1a843283215.webp\" alt=\"Architecture Sample\"></h2>\n<h2>Architecture &amp; Technical Implementation</h2>\n<p>Built with <strong>Python 3.13+</strong>, the system employs a modular, event-driven architecture designed around four core execution pillars:</p>\n<h3>1. Natural Language Processing Pipeline</h3>\n<ul>\n<li>AI-powered prompt interpretation layer that translates user intent into structured shell commands  </li>\n<li>Custom data types for command representation and validation  </li>\n<li>Context-aware command generation with semantic understanding of system state</li>\n</ul>\n<h3>2. Execution Engine &amp; Orchestration</h3>\n<p>The executor module implements a sophisticated command lifecycle management system:</p>\n<ul>\n<li>Run ID generation and tracking for execution auditing  </li>\n<li>Run hash computation for command deduplication and caching  </li>\n<li>Atomic execution cycles with comprehensive logging and state persistence  </li>\n<li>File-based run tracking system enabling rollback and debugging capabilities</li>\n</ul>\n<h3>3. Security-First Design</h3>\n<ul>\n<li>Secrets management module implementing secure credential handling  </li>\n<li>Environment variable isolation and sandboxing  </li>\n<li>User-controlled execution modes: <strong>autonomous</strong> vs. <strong>approval-gated</strong> workflows  </li>\n<li>Command validation and sanitization before shell execution</li>\n</ul>\n<h3>4. DevOps Integration</h3>\n<ul>\n<li>Dockerized testing infrastructure for reproducible test environments  </li>\n<li>GitHub Actions CI/CD pipeline with automated <code>pytest</code> execution on pull requests  </li>\n<li>Modular test suite with isolated unit tests for each system component  </li>\n<li>Workflow automation for PR validation (automated testing workflows #5–#8)</li>\n</ul>\n<hr>\n<h2>Development Methodology &amp; Engineering Rigor</h2>\n<p>The project demonstrates professional software engineering practices:</p>\n<ul>\n<li><p><strong>Test-Driven Development</strong><br>Comprehensive <code>pytest</code> suite with 13+ passing unit tests for configuration management  </p>\n</li>\n<li><p><strong>Containerization</strong><br>Docker-based testing environment ensuring consistency across development and CI/CD  </p>\n</li>\n<li><p><strong>Version Control Discipline</strong><br>38+ atomic commits with clear SCRUM ticket references (e.g. <code>SCRUM-28</code>, <code>SCRUM-31</code>)  </p>\n</li>\n<li><p><strong>Pull Request Workflow</strong><br>8 merged PRs with automated quality gates</p>\n</li>\n</ul>\n<hr>\n<h2>Key Technical Achievements</h2>\n<ul>\n<li><p><strong>Command Generation Cycle</strong><br>End-to-end natural language → shell command translation with structured output formatting  </p>\n</li>\n<li><p><strong>Configuration Management</strong><br>Type-safe configuration module with comprehensive test coverage, handling environment variables, secrets, and runtime parameters  </p>\n</li>\n<li><p><strong>Logging Infrastructure</strong><br>Structured logging system capturing execution traces, enabling debugging and audit trails for AI-generated commands  </p>\n</li>\n<li><p><strong>Orchestration Layer</strong><br>Multi-stage command execution pipeline with error detection, recovery mechanisms, and execution state management  </p>\n</li>\n<li><p><strong>CLI Installation Framework</strong><br>Custom installer (<code>install.py</code>) with dependency management and environment setup automation</p>\n</li>\n</ul>\n<hr>\n<h2>Technical Stack</h2>\n<ul>\n<li><strong>Core:</strong> Python 3.13, Typer (CLI framework)  </li>\n<li><strong>AI / ML:</strong> LLM integration for natural language understanding (model selection abstracted for flexibility)  </li>\n<li><strong>Testing:</strong> pytest, Docker test containers  </li>\n<li><strong>CI/CD:</strong> GitHub Actions with automated PR validation  </li>\n<li><strong>Development:</strong> Modular package structure (<code>client/</code>, <code>modules/</code>, <code>tests/</code>)</li>\n</ul>\n<hr>\n<h2>Project Maturity &amp; Future Vision</h2>\n<p>The repository contains <strong>~10,000+ lines of code</strong> across modular components, demonstrating a transition from prototype to production-ready system. The codebase includes:</p>\n<ul>\n<li>Robust error handling and recovery mechanisms  </li>\n<li>Comprehensive documentation (README files at project and client levels)  </li>\n<li>Clean separation of concerns (executor, logger, config, secrets as independent modules)  </li>\n<li>Extensible architecture for adding new command interpreters and execution backends</li>\n</ul>\n<hr>\n<h2>Impact &amp; Implications</h2>\n<p>This project tackles a fundamental human–computer interaction challenge: making powerful CLI tools accessible without sacrificing control or security. By combining AI-driven natural language understanding with rigorous software engineering practices, <strong>LEO-LOCAL</strong> demonstrates how next-generation developer tools can enhance productivity while maintaining professional standards for reliability, security, and maintainability.</p>\n",
    "text": "LEO-LOCAL: AI-Powered Natural Language Shell Interface Vision &amp; Technical Overview LEO-LOCAL represents an ambitious bridge between natural language processing and system administration, transforming how developers interact with command-line environments. The project envisions a future where shell operations are democratized through conversational AI, eliminating the cognitive overhead of memorizing complex command syntax while maintaining the precision and power of traditional CLIs. Architecture &amp; Technical Implementation Built with Python 3.13+ , the system employs a modular, event-driven architecture designed around four core execution pillars: 1. Natural Language Processing Pipeline AI-powered prompt interpretation layer that translates user intent into structured shell commands Custom data types for command representation and validation Context-aware command generation with semantic understanding of system state 2. Execution Engine &amp; Orchestration The executor module implements a sophisticated command lifecycle management system: Run ID generation and tracking for execution auditing Run hash computation for command deduplication and caching Atomic execution cycles with comprehensive logging and state persistence File-based run tracking system enabling rollback and debugging capabilities 3. Security-First Design Secrets management module implementing secure credential handling Environment variable isolation and sandboxing User-controlled execution modes: autonomous vs. approval-gated workflows Command validation and sanitization before shell execution 4. DevOps Integration Dockerized testing infrastructure for reproducible test environments GitHub Actions CI/CD pipeline with automated pytest execution on pull requests Modular test suite with isolated unit tests for each system component Workflow automation for PR validation (automated testing workflows #5–#8) Development Methodology &amp; Engineering Rigor The project demonstrates professional software engineering practices: Test-Driven Development Comprehensive pytest suite with 13+ passing unit tests for configuration management Containerization Docker-based testing environment ensuring consistency across development and CI/CD Version Control Discipline 38+ atomic commits with clear SCRUM ticket references (e.g. SCRUM-28 , SCRUM-31 ) Pull Request Workflow 8 merged PRs with automated quality gates Key Technical Achievements Command Generation Cycle End-to-end natural language → shell command translation with structured output formatting Configuration Management Type-safe configuration module with comprehensive test coverage, handling environment variables, secrets, and runtime parameters Logging Infrastructure Structured logging system capturing execution traces, enabling debugging and audit trails for AI-generated commands Orchestration Layer Multi-stage command execution pipeline with error detection, recovery mechanisms, and execution state management CLI Installation Framework Custom installer ( install.py ) with dependency management and environment setup automation Technical Stack Core: Python 3.13, Typer (CLI framework) AI / ML: LLM integration for natural language understanding (model selection abstracted for flexibility) Testing: pytest, Docker test containers CI/CD: GitHub Actions with automated PR validation Development: Modular package structure ( client/ , modules/ , tests/ ) Project Maturity &amp; Future Vision The repository contains ~10,000+ lines of code across modular components, demonstrating a transition from prototype to production-ready system. The codebase includes: Robust error handling and recovery mechanisms Comprehensive documentation (README files at project and client levels) Clean separation of concerns (executor, logger, config, secrets as independent modules) Extensible architecture for adding new command interpreters and execution backends Impact &amp; Implications This project tackles a fundamental human–computer interaction challenge: making powerful CLI tools accessible without sacrificing control or security. By combining AI-driven natural language understanding with rigorous software engineering practices, LEO-LOCAL demonstrates how next-generation developer tools can enhance productivity while maintaining professional standards for reliability, security, and maintainability.",
    "assets": [
      "/projects/leo_image_a1a843283215.webp"
    ]
  }
];
